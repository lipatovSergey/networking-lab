# Лабораторная 5 — Load Testing (Нагрузочное тестирование)

## Цель лабораторной

Понять, **как backend ведёт себя под нагрузкой**, и научиться интерпретировать базовые метрики производительности:

- latency
- percentiles (p50 / p95 / p99)
- requests per second (RPS)
- bytes per second

Лабораторная показывает разницу между:

- быстрым кодом
- и быстрым HTTP-сервисом.

---

## Инструмент

Использовался **autocannon** — инструмент для генерации HTTP-нагрузки.

Autocannon:

- создаёт множество клиентов,
- отправляет реальные HTTP-запросы,
- измеряет поведение сервера как «чёрного ящика».

Важно: autocannon **не бенчмарчит код**, он нагружает **HTTP-интерфейс целиком**.

---

## Базовая точка (baseline)

Тестировался простой HTTP-endpoint:

- маленький ответ
- без БД
- без задержек

### Наблюдения

- очень высокий RPS
- минимальная latency
- узких мест не видно

### Вывод

Быстрый endpoint без I/O почти ни во что не упирается.

---

## Эксперимент A — большой ответ (`/big`)

Endpoint возвращает большой payload (~1 MB).

### Наблюдения

- RPS снизился почти в 2 раза
- bytes/sec тоже снизился
- latency осталась низкой

### Выводы

- bottleneck — **сеть и сокеты**, а не CPU
- большие ответы дольше удерживают соединения
- throughput ограничен пропускной способностью
- быстрый код ≠ масштабируемый сервис

---

## Эксперимент B — искусственная задержка (`/wait`)

Endpoint:

- маленький ответ
- задержка через `setTimeout` (имитация БД)

### Вариант 100 ms

Наблюдения:

- latency ≈ 100 ms
- RPS ≈ 100 при 10 connections
- bytes/sec низкий

### Вариант 10 ms

Наблюдения:

- latency ≈ 10 ms
- RPS ≈ 900 при 10 connections

### Ключевой вывод

Для I/O-bound операций:

```
RPS ≈ connections / latency
```

Latency определяется самой операцией, а throughput — количеством одновременных запросов.

---

## Эксперимент C — влияние concurrency (`-c`)

Тестировался `/wait` с задержкой 100 ms при разном количестве connections.

### Результаты

| Connections | Latency | Req/sec |
| ----------- | ------- | ------- |
| 1           | ~100 ms | ~10     |
| 10          | ~100 ms | ~100    |
| 50          | ~100 ms | ~500    |

### Выводы

- latency почти не меняется
- RPS и bytes/sec растут почти линейно
- масштабирование происходит за счёт concurrency
- JS остаётся однопоточным, но запросы обрабатываются конкурентно

---

## Ключевые понятия

### Latency

Время от отправки HTTP-запроса до получения полного ответа клиентом.

### Percentiles

- **p50** — медианный запрос
- **p95 / p97.5** — почти худшие случаи
- **p99** — хвост, критичный для UX

Среднее значение latency не отражает реальную картину.

---

## I/O и модель Node.js

- I/O — любое взаимодействие с внешним миром (БД, сеть, диск)
- I/O не блокирует JS-поток
- Пока один запрос ждёт БД, JS может обслуживать другие
- Это называется **concurrency**, а не параллельность

Node.js:

- один JS-поток
- множество одновременных I/O-ожиданий
- event loop переключается между готовыми задачами

---

## Главные выводы лабораторной

- Производительность backend — это не только скорость кода
- Bottleneck может быть:
  - сеть
  - I/O ожидание
  - сокеты

- Большие ответы ухудшают throughput
- I/O-bound сервисы масштабируются через concurrency
- Latency — свойство операции, RPS — свойство системы
- Node.js эффективен для I/O-bound задач, но плохо переносит CPU-блокировки
